{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake RL - Results Analysis\n",
    "\n",
    "This notebook analyzes training results and generates publication-ready figures for the IB Extended Essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data\n",
    "\n",
    "Load CSV logs from multiple training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_run(run_path):\n",
    "    \"\"\"Load progress.csv from a run directory.\"\"\"\n",
    "    csv_file = Path(run_path) / \"progress.csv\"\n",
    "    if csv_file.exists():\n",
    "        return pd.read_csv(csv_file)\n",
    "    else:\n",
    "        print(f\"Warning: {csv_file} not found\")\n",
    "        return None\n",
    "\n",
    "# Example: Load your runs here\n",
    "runs = {\n",
    "    'Baseline': load_run('runs/baseline'),\n",
    "    'Distance Shaping': load_run('runs/distance_shaping'),\n",
    "    'Frame Stacking': load_run('runs/frame_stacking'),\n",
    "}\n",
    "\n",
    "# Remove None values\n",
    "runs = {k: v for k, v in runs.items() if v is not None}\n",
    "\n",
    "print(f\"Loaded {len(runs)} runs:\")\n",
    "for name in runs.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Curves\n",
    "\n",
    "Plot episode reward over time for all treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, df in runs.items():\n",
    "    if 'rollout/ep_rew_mean' in df.columns and 'time/total_timesteps' in df.columns:\n",
    "        # Smooth with moving average\n",
    "        window = 10\n",
    "        x = df['time/total_timesteps']\n",
    "        y_raw = df['rollout/ep_rew_mean']\n",
    "        y_smooth = y_raw.rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        ax.plot(x, y_smooth, label=name, linewidth=2)\n",
    "        ax.plot(x, y_raw, alpha=0.2, linewidth=1, color=ax.lines[-1].get_color())\n",
    "\n",
    "ax.set_xlabel('Timesteps', fontsize=12)\n",
    "ax.set_ylabel('Episode Reward (mean)', fontsize=12)\n",
    "ax.set_title('Learning Curves Comparison', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Efficiency\n",
    "\n",
    "Compare how quickly each treatment learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area under curve (AUC) for each run\n",
    "efficiency = {}\n",
    "\n",
    "for name, df in runs.items():\n",
    "    if 'rollout/ep_rew_mean' in df.columns and 'time/total_timesteps' in df.columns:\n",
    "        # Use trapezoidal rule for AUC\n",
    "        x = df['time/total_timesteps'].values\n",
    "        y = df['rollout/ep_rew_mean'].values\n",
    "        auc = np.trapz(y, x)\n",
    "        efficiency[name] = auc / x[-1]  # Normalize by total timesteps\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "names = list(efficiency.keys())\n",
    "values = list(efficiency.values())\n",
    "\n",
    "ax.bar(names, values, alpha=0.8)\n",
    "ax.set_ylabel('Mean Reward (AUC normalized)', fontsize=12)\n",
    "ax.set_title('Sample Efficiency Comparison', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_efficiency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSample Efficiency (higher is better):\")\n",
    "for name, val in efficiency.items():\n",
    "    print(f\"  {name}: {val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Performance\n",
    "\n",
    "Compare the final performance (last 100 timesteps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_last = 100\n",
    "final_stats = {}\n",
    "\n",
    "for name, df in runs.items():\n",
    "    if 'rollout/ep_rew_mean' in df.columns:\n",
    "        last_data = df.tail(n_last)['rollout/ep_rew_mean']\n",
    "        final_stats[name] = {\n",
    "            'mean': last_data.mean(),\n",
    "            'std': last_data.std(),\n",
    "            'min': last_data.min(),\n",
    "            'max': last_data.max(),\n",
    "        }\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "names = list(final_stats.keys())\n",
    "means = [stats['mean'] for stats in final_stats.values()]\n",
    "stds = [stats['std'] for stats in final_stats.values()]\n",
    "\n",
    "ax.bar(names, means, yerr=stds, capsize=5, alpha=0.8)\n",
    "ax.set_ylabel('Episode Reward (mean ± std)', fontsize=12)\n",
    "ax.set_title(f'Final Performance (last {n_last} samples)', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Performance Statistics:\")\n",
    "for name, stats_dict in final_stats.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean: {stats_dict['mean']:.2f} ± {stats_dict['std']:.2f}\")\n",
    "    print(f\"  Range: [{stats_dict['min']:.2f}, {stats_dict['max']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Tests\n",
    "\n",
    "Perform t-tests to determine if differences are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline with each treatment\n",
    "if 'Baseline' in runs:\n",
    "    baseline_data = runs['Baseline'].tail(n_last)['rollout/ep_rew_mean']\n",
    "    \n",
    "    print(\"Statistical Comparisons (vs. Baseline):\\n\")\n",
    "    \n",
    "    for name, df in runs.items():\n",
    "        if name != 'Baseline' and 'rollout/ep_rew_mean' in df.columns:\n",
    "            treatment_data = df.tail(n_last)['rollout/ep_rew_mean']\n",
    "            \n",
    "            # Independent samples t-test\n",
    "            t_stat, p_value = stats.ttest_ind(baseline_data, treatment_data)\n",
    "            \n",
    "            # Cohen's d (effect size)\n",
    "            pooled_std = np.sqrt((baseline_data.std()**2 + treatment_data.std()**2) / 2)\n",
    "            cohens_d = (treatment_data.mean() - baseline_data.mean()) / pooled_std\n",
    "            \n",
    "            print(f\"{name}:\")\n",
    "            print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            print(f\"  Cohen's d: {cohens_d:.3f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                print(f\"  ✓ Significant difference (p < 0.05)\")\n",
    "            else:\n",
    "                print(f\"  ✗ No significant difference (p >= 0.05)\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No 'Baseline' run found for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Stability\n",
    "\n",
    "Analyze variance in episode rewards over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, df in runs.items():\n",
    "    if 'rollout/ep_rew_mean' in df.columns and 'time/total_timesteps' in df.columns:\n",
    "        # Calculate rolling standard deviation\n",
    "        window = 50\n",
    "        x = df['time/total_timesteps']\n",
    "        y_std = df['rollout/ep_rew_mean'].rolling(window=window, min_periods=1).std()\n",
    "        \n",
    "        ax.plot(x, y_std, label=name, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Timesteps', fontsize=12)\n",
    "ax.set_ylabel('Episode Reward Std Dev (rolling)', fontsize=12)\n",
    "ax.set_title('Training Stability (lower is more stable)', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_stability.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Table\n",
    "\n",
    "Create a comprehensive summary table for the EE report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for name, df in runs.items():\n",
    "    if 'rollout/ep_rew_mean' in df.columns:\n",
    "        last_data = df.tail(n_last)['rollout/ep_rew_mean']\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Treatment': name,\n",
    "            'Final Reward': f\"{last_data.mean():.2f} ± {last_data.std():.2f}\",\n",
    "            'Sample Efficiency': f\"{efficiency.get(name, 0):.3f}\",\n",
    "            'Stability (Std)': f\"{last_data.std():.2f}\",\n",
    "            'Total Timesteps': f\"{int(df['time/total_timesteps'].iloc[-1]):,}\",\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n=\" * 80)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save to CSV for LaTeX/Word\n",
    "summary_df.to_csv('results_summary.csv', index=False)\n",
    "print(\"\\n✓ Summary saved to results_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export All Figures\n",
    "\n",
    "Run this cell to regenerate all figures at publication quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All figures have been saved:\")\n",
    "print(\"  - learning_curves.png\")\n",
    "print(\"  - sample_efficiency.png\")\n",
    "print(\"  - final_performance.png\")\n",
    "print(\"  - training_stability.png\")\n",
    "print(\"  - results_summary.csv\")\n",
    "print(\"\\n✓ Ready for use in your IB Extended Essay!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}